---
layout: page
title: 自然语言处理——隐马尔可夫模型
categories: [深度学习]
tags: [dl, nlp]
keywords: 
description: 摘要描述
mathjax: true
---

## 前言

### 词性（Part-of-Speech）

POS 是依据语法功能划分，是词语在区别词类时用到的属性。

### 词性标注的方法

1. rule-based

   语言专家根据词法及语言学知识编制的规则。

2. learning-based

   从专家标注的语料库中学习到用于自动标注的模型

   * 统计模型：隐马尔可夫模型（HMM），条件随机域模型（CRF），神经网络模型（NN）
   * 规则学习：基于转换的学习（TBL）

### 符号规定

| 符号  | 含义                                  |
| :---: | :------------------------------------ |
|  $N$  | 训练数据中的句子总数                  |
| $O_i$ | 第 i 个句子（词序列）                 |
| $o_i$ | 某句子中的第 i 个词                   |
| $Q_i$ | 第 i 个句子对应的词性标注（词性序列） |
| $q_i$ | 某句子中的第 i 个词对应的词性         |

### 基于统计语言模型的词性标注基本模型

$$
\max_Q P(Q|O)
$$

由于语料库不可能包含所有可能出现的句子，故应得到一个更加宽泛的的表达式。利用贝叶斯法则得等价模型
$$
\max_Q P(O|Q)P(Q)
$$

## 隐马尔可夫模型（HMM）

### 假设

1. 一阶马尔可夫假设，即语义相关性只涉及到前面 1 个词（也可设为 2 阶或 3 阶）：$P(Q) = P(q_1)P(q_2|q_1)...P(q_N|q_{N-1})$；
2. 观测独立性假设，当前时刻的观测值仅与当前时刻的不可观测量的值（状态值）有关，与其他时刻的观测值无关；即单词$o_i$对应的$q_i$不受其他单词影响，即$P(o_i|q_i)$相互独立：$P(O|Q)=\prod P(o_i|q_i)$，故在该模型中，观测独立性假设也可称为条件独立性假设。注意状态之间独立性并不成立，

### 模型

$$
\max_Q \prod P(o_i|q_i) * \prod P(q_j|q_{j-1}),
$$

其中$P(o_i|q_i)$被称为发射概率，是通过统计每个单词在语料库中的出现情况得到的。对于因某个单词没有在语料库中出现导致发射概率为 0 进而导致整个句子出现概率为 0 的情况，须做一些平滑处理。

### 求解

#### 由模型定义求解（暴力遍历）

对于给定的观测序列，求所有可能状态序列的概率，并将最大概率的状态序列最为所求结果。设观测序列长度为 T ，可选状态数为 M，可选观测数为 N，首先在最一开始时由初始状态概率向量 $\pi$求出后续 T-1 个状态概率向量$i_t = \pi A^{t-1}$ ，那么一个可能状态序列的概率为，对一个句子的词性标注的时间复杂度为。





### 总结

使用离散时间点、离散状态，并做了马尔可夫假设，由此系统产生了马尔可夫过程的模式，它包含一个$\pi$向量和一个状态转移矩阵。

隐马尔科夫模型是在一个标准的马尔科夫过程中引入一组观察状态，以及该组观察状态与隐藏状态间的概率关系。