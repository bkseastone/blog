---
layout: page
title: 自然语言处理:基础
categories: [深度学习]
tags: [dl, nlp]
keywords: 
description: 摘要描述
mathjax: true
---

## 术语

* 词典（词表）：目标语言的所有单词，记为$\mathcal{D}$。
* 语料库（Corpus）：即手中的数据集，记为$\mathcal{C}$。
* 上下文（context）：词汇所处的共现词、语境、前邻词、后邻词等词汇出现的篇章语境信息。
* 词符（tocken）：目标语言中一个词的标记，即指一个单词。
* 词串：一系列词符前后连接成串。
* 词串共现：两个词串在同一个句子中。
* 历史词：出现在该词符之前的所有词。

## 统计语言模型（statistical language model）

统计语言模型是基于大数定律，结合贝叶斯公式，利用语料库来计算一个句子（或词串）的概率的。n 个词串共现的概率为：
$$
P(W) = P(w_1)P(w_2|w_1)...P(w_n|w_1,w_2,...,w_{n-1})
\\
where, \quad P(w_i) = \frac{count(w_i)}{count(\mathcal{C})}
\\
P(w_i|w_1,...,w_{i-1}) = \frac{count(w_1,...,w_{i-1},w_i)}{count(w_1,...,w_{i-1})}
$$
求解该模型的方法有很多，n-gram 模型、决策树、最大熵模型、最大熵马尔科夫模型、条件随机场、神经网络等。

当所有的概率值都计算好之后便存储起来，下次需要计算一个词串的概率时，只需找到相关的概率参数，将之连乘即可。

### n-gram

添加 n-1 阶马尔科夫假设，得到 n-gram 模型（假设为 3-gram）：
$$
P(w_i|w_1,...,w_{i-1}) = \frac{count(w_{i-2},w_{i-1},w_i)}{count(w_{i-2},w_{i-1})}
$$


当 n 越大：

1. 模型对历史词的关联性越强，故可区别性越好（模型复杂度越高）；
2. 因模型复杂度呈指数级增高，大数定理的可靠性便越来越差（可理解为一种过拟合现象，因为模型复杂度相对于样本复杂度过高）。


#### 数据稀疏问题

> 产生该问题的根本原因是采用了统计语言模型。

1. 语料库中可能出现$count(w_1,...,w_{i-1},w_i) = 0$的情况，即该词串永远不会出现，但不应认为$P(w_i|w_1,...,w_{i-1})=0$；
2. 语料库中可能出现$count(w_1,...,w_{i-1}) = count(w_1,...,w_{i-1},w_i)$的情况，但不应认为$P(w_i|w_1,...,w_{i-1})=$1。

这两种问题称为数据稀疏问题，该问题的出现与语料库的大小无关，由[Zipf定律](https://www.gelbukh.com/CV/Publications/2001/CICLing-2001-Zipf.htm)知，在自然语言的语料库里，一个单词出现的频率与它在频率表里的排名成反比，故增大语料库依然无法解决数据稀疏问题。

在 n-gram 模型中，当 n 达到一定值后，即使样本复杂度足够，由于数据稀疏问题，n 越大，性能反而越差。


#### 平滑技术

平滑技术是针对数据稀疏问题引入的技术，常用的有：

* 平滑：拉普拉斯平滑（加一平滑）、Lidstone 平滑（加 delta 平滑）、good-turing 平滑。
* 回退：backoff、interpolation（软回退）、kneser-ney smoothing。


### NNLM

Bengio et. al（2003）



## 词符表示（tocken）

### discrete representation 

独热（one-hot representation）：可认为是一种基于符号的词表示方法。

### distributed representation

> 基于分布式相似度的表示，英文全称为 distributional similarity based representations。
>
> You shall know a word by the company it keeps.    -- J.R.Firth 1957:11

#### 共现矩阵（cooccurence matrix）

用上下文来表示一个单词的方法。有两种计算方法：

* 基于整个段落的，又称为潜在语义分析。
* 基于窗口的（窗口内一般为5~10个单词，共现矩阵的行是窗口个数，列为所有不重复单词个数），优点是将语义和语法都考虑了进去。

缺点是维度灾难，常用 SVD 来压缩矩阵以实现降维。

#### 词向量

