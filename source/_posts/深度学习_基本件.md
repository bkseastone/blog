---
layout: page
title: 深度学习:基本件
categories: [深度学习]
tags: [dl]
keywords: 
description: 摘要描述
mathjax: true
---

# [Attention](<https://xdaping.github.io/posts/nlp-self-attention-models.html>)

![attention计算过程](https://img.vim-cn.com/98/25009c5201091fbab8ffd2f2a92b353d2ae523.jpg)

其中query为目标语言端的h，key为源语言端的h，value也为源语言端的h，通过点积等方法计算相似度后得到该时刻t的a，将value与a做加权求和后得到attention value，即得到该t时刻目标语言端的h。

简单来说就是将当前时刻的目标语言与源语言的各个向量求个相似度，用来对源语言的各个向量做加权求和，从而通过attention机制实现源语言与目标语言的对齐。

# 损失函数

## 应对样本不均衡

### focal loss

$$
loss = \{^{-\alpha (1-y')^\gamma \log y', y=1}
_{-(1-\alpha)(y')^\gamma \log (1-y'), y=0}.
$$

* focal loss 用于在训练过程中提高难分样本的权重，降低易分样本的权重，即尽可能地将计算资源用在难分样本上，从而提高模型的收敛能力；或者可认为是 AdaBoost 在深度学习上的应用（resnet 是 GradientBoost 在深度学习上的应用）。
* 其中 $\alpha$ 用以平衡正负样本数据集本身的不平衡，与 focal loss 无关。
* $\gamma$ 表示对难分样本关注的程度。

#### 应用

##### 一阶目标检测

对于二阶目标检测算法，有特征重采样过程，可在该过程中通过固定正负样本比例或在线困难样本挖掘（OHEM）使得前景和背景相对平衡；而一阶目标检测算法最多只能在数据集上做固定正负样本比例的处理，训练会被大量易分样本主导，导致模型难以收敛至更优解，focal loss 用 Boost 的方法解决了这个问题。

